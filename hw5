# Amanda Chang
# Code was done in Google Colab

!pip install spacy
!pip install newsapi-python

!python -m spacy download en_core_web_lg

import spacy #had to add in this line for code to work
from newsapi import NewsApiClient #had to add in this line for code to work

nlp_eng = en_core_web_lg.load()
newsapi = NewsApiClient(api_key='fefd6986358744d29ad9185cf6fea486')

articles = []
#date and page were modified. 20 articles in ea of 5 pages for a total 100 articles
for i in range(1,6,1):
  articles.append(newsapi.get_everything(q='coronavirus', language='en', from_param='2021-09-29', to='2021-10-19', sort_by='relevancy', page=i))
# testing purposes
# for x in articles:
#   print(x)

import pickle
filename = 'articlesCOVID.pckl'
pickle.dump(articles, open(filename, 'wb'))
filename = 'articlesCOVID.pckl'
loaded_model = pickle.load(open(filename, 'rb'))
filepath = '/content/articlesCOVID.pckl'
pickle.dump(loaded_model, open(filepath, 'wb'))

# code from website gave errors so I made adjustments
titles = []
dates = []
descriptions = []
content = []

for i, article in enumerate(articles):
    for x in article['articles']:
        titles.append(x['title'])
        dates.append(x['publishedAt'])
        descriptions.append(x['description'])
        content.append(x['content'])
        
# add the data for 100 articles to data dictionary that will be put into a data frame
data = {
    'title' : titles,
    'date' : dates,
    'desc': descriptions,
    'content' : content
}

import pandas as pd
df = pd.DataFrame(data)
df = df.dropna()
df.head()

# takes the content (text) and returns a list of keywords from the text
def get_keywords_eng(text):
  result = []
  # i assume we have to add these lists
  punctuation = ['.', '?', '!']
  pos_tag = ['VERB', 'NOUN', 'PROPN']
  # found out that this was needed
  doc = nlp_eng(text)
  for token in doc:
    if (token.text in nlp_eng.Defaults.stop_words or token.text in punctuation):
      continue
    if (token.pos_ in pos_tag):
      result.append(token.text)
  return result
  
from collections import Counter
results = []

for content in df.content.values:
    results.append([('#' + x[0]) for x in Counter(get_keywords_eng(content)).most_common(5)])
df['keywords'] = results

df.to_csv(r'/content/data.csv')
